{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=5>Tennessee Fuel Quality Analysis</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Date Published**: 2019/07/31\n",
    "* **Collaborators**: Kate Hayes & Misha Berrien \n",
    "* **Data Source**: ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "\n",
    "src_dir = os.path.join(os.getcwd(), '..', '..', 'src')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# helper functions \n",
    "from d03_processing.feature_engineering import process_data_for_model_building\n",
    "from d04_modelling.modelling import get_model_pvalue\n",
    "from d03_processing.Time_series_cleaning import date_results_df_creator\n",
    "from d03_processing.Time_series_cleaning import volatilty_ASTM_df_creator\n",
    "\n",
    "\n",
    "# Load the \"autoreload\" extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of Tennessee's Department of Agriculture (TDA) maintaines a fuel quality program. The state inspects all places where fuel is sold/ distributed including gas stations, terminals, airports etc. on an annual basis. This results of these routine tests as well as follow up tests for complaints are available. Tennessee has a Sunshine law (public records law) that allows anyone to request any state record through the right legal channels. The data set was aquired in this manor. We sent an official Tennessee records request for the fuel qulity data for the last 5 years. It came in the form of routine inspections and complaints one set for each years, so 10 Excel files in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following key metrics were descriptions given to us by the TDA Fuel Quality Manager. The main thing that we were inspecting in this project was the prediction of pass/fail rates and volatility properties with the seasons. \n",
    "\n",
    "Volatility is an important property of gasoline because it must be able to vaporize before combusting in an engine. Three characteristics are used to measure the volatility of gasoline and evaluate suitability: vapor pressure, vapor-liquid ratio, and the distillation temperature at which 50% of the fuel is evaporated.\n",
    "\n",
    "\n",
    "* Vapor pressure is a measure of the amount of vapor that is produced by a gasoline sample at 37.8°C (100°F). Vapor pressure most affects an engine’s ease of starting. The vapor pressure specification is a maximum allowable limit reported in kilopascals. The pressure must be high enough to promote easy starting but not too high to contribute to excessive emissions or vapor lock -  the presence of too much vapor that leads to loss of engine power or rough operation.\n",
    "\n",
    "\n",
    "* Vapor-liquid ratio is the ratio of the volume of vapor to the volume of liquid at atmospheric pressure. The vapor-liquid ratio specification is a minimum allowable limit reported in degrees Celsius. The reported value is the temperature at which the vapor-liquid ratio is equal to 20 (20:1 vol/vol), the approximate temperature at which engine problems may occur. Vapor-liquid ratio is used to evaluate a gasoline sample’s tolerance to changes in temperature. A noncompliant test result (too low) may lead to vapor lock or hot fuel handling problems, as evidenced by loss of power while accelerating or idling.\n",
    "\n",
    "\n",
    "* Distillation measures the temperature range across which a sample is heated to fully evaporate. The temperature at which 50% of a sample is evaporated (T50) relates to the driveability (smoothness and ease of driving) and idling characteristics for the fuel. T50 most similarly relates to how a fuel performs under continuous activity (not starting or warming up). T50 has minimum and maximum allowable limits reported in degrees Celsius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats/ shortcomings/ issues (whatever you would name this section) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Datasets \n",
    "\n",
    "The fuel volatility specifications change throughout the year based on outdoor temperature. Therse specifications are set by the ASTM. The ASTM standards were joined onto the five year result data frame. in order to assess the seasonality of the data. A Dicky Fuller test and selective seaonal decomposition was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Sample      Prod DateSampled               Grade  \\\n",
      "DateSampled                                                       \n",
      "2015-11-23   61916134  Gasoline  2015-11-23  Mid Grade Unleaded   \n",
      "2015-11-23   61916134  Gasoline  2015-11-23  Mid Grade Unleaded   \n",
      "2015-11-23   61916134  Gasoline  2015-11-23  Mid Grade Unleaded   \n",
      "2015-11-23   61916134  Gasoline  2015-11-23  Mid Grade Unleaded   \n",
      "2015-11-23   61916134  Gasoline  2015-11-23  Mid Grade Unleaded   \n",
      "\n",
      "                          Supplier       FacilityName  \\\n",
      "DateSampled                                             \n",
      "2015-11-23   Marathon Petroleum Lp  Circle K #2723609   \n",
      "2015-11-23   Marathon Petroleum Lp  Circle K #2723609   \n",
      "2015-11-23   Marathon Petroleum Lp  Circle K #2723609   \n",
      "2015-11-23   Marathon Petroleum Lp  Circle K #2723609   \n",
      "2015-11-23   Marathon Petroleum Lp  Circle K #2723609   \n",
      "\n",
      "                                        SiteAddress              Test   Units  \\\n",
      "DateSampled                                                                     \n",
      "2015-11-23   198 Haywood Ln \\r\\nnashville, Tn 37211   Antiknock Index           \n",
      "2015-11-23   198 Haywood Ln \\r\\nnashville, Tn 37211       API Gravity           \n",
      "2015-11-23   198 Haywood Ln \\r\\nnashville, Tn 37211              DIPE  Vol. %   \n",
      "2015-11-23   198 Haywood Ln \\r\\nnashville, Tn 37211  Distillation 10%  Deg. C   \n",
      "2015-11-23   198 Haywood Ln \\r\\nnashville, Tn 37211  Distillation 20%  Deg. C   \n",
      "\n",
      "            Method Result MinResult MaxResult Compliance  \n",
      "DateSampled                                               \n",
      "2015-11-23   R+M/2   89.8        87                    Y  \n",
      "2015-11-23   D4052   60.9                           None  \n",
      "2015-11-23   D4815     ND                           None  \n",
      "2015-11-23     D86   43.1                              Y  \n",
      "2015-11-23     D86   51.0                           None  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'full_volatility_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-16134c905b79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvolatility_and_astm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvolatilty_ASTM_df_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/02_intermediate/routine_clean.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../../data/01_raw/ASTM_fuel.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvolatility_and_astm_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Coding/Fuel_quality/tenessee-fuel-quality-analysis/notebooks/04_reports/../../src/d03_processing/Time_series_cleaning.py\u001b[0m in \u001b[0;36mvolatilty_ASTM_df_creator\u001b[0;34m(routine_csv, ASTM_csv)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mfull_volitility_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datesampled_day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_volitility_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DateSampled'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfull_volitility_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datesampled_month_day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_volitility_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datesampled_month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfull_volitility_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datesampled_day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;31m#full_volitility_df.rename(columns={'datesampled_month_day' : 'Date'}, inplace = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_volitility_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#(full_volatility_df.head())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_volatility_df' is not defined"
     ]
    }
   ],
   "source": [
    "volatility_and_astm_df = volatilty_ASTM_df_creator('../../data/02_intermediate/routine_clean.csv', '../../data/01_raw/ASTM_fuel.csv')\n",
    "volatility_and_astm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillaltion_df = date_results_df_creator(volatility_and_astm_df, 'Distillation 50%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vapor_pressure_df = date_results_df_creator(volatility_and_astm_df, 'Vapor Pressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vapor_liquid_df = date_results_df_creator(volatility_and_astm_df, 'Vapor-Liquid Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicky-Fuller Stationality Tests\n",
    "Once the proper dataframes were created a Dicky Fuller Test was added to assess for stationarity. All of the tests were stationary above a 1% critical value. Two failed at 1% but that was due to extreme sample failures. The average, maximum, and minimum values for each day were taken becasue there was more than one value for each date and time series analysis requires there to be no more than one value for each date. Assessing at these points allowed differing seasonal trends to be seen and the outliers on either side of the spectrum to become more obvious. It is not the most informative thing to say that the value of one sample in Knoxville averaged with another in Nashville can tell the user anything about the trends. It can disguies a lot of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(Distillation_50.groupby('DateSampled').mean(), 'Distillation 50 % Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(Vapor_Liquid.groupby('DateSampled').mean(), 'Vapor Liquid Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(Vapor_Pressure.groupby('DateSampled').mean(), 'Vapor Pressure Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(Distillation_50.groupby('DateSampled').max(), 'Distillation 50 % Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(Vapor_Liquid.groupby('DateSampled').max(), 'Vapor Liquid Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(Vapor_Pressure.groupby('DateSampled').mean(), 'Vapor Pressure Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(Distillation_50.groupby('DateSampled').min(), 'Distillation 50 % Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(Vapor_Liquid.groupby('DateSampled').min(), 'Vapor Liquid Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(Vapor_Pressure.groupby('DateSampled').min(), 'Vapor Pressure Mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Decomposition\n",
    "A time series decomposition on the average vapor pressure was done. This was selected to do the decomposition becasue it had an easy to see trend of seasonality with few outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make A resampled vapor pressure data frame that has frequency by buisness days and the missing dates are upsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_decompose(resampled_vp, 'Average Vapor Pressue Seasonal Decomposition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency was done via buisness days so becasue of that it does not pull out the quarterly trend data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Datasets\n",
    "\n",
    "The helper functions for cleaning/ processing the dataset can be found in the src/d03_processing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasoline_proc = pd.read_csv('../../data/03_processed/gasoline_processed.csv')\n",
    "astm = pd.read_csv('../../data/01_raw/ASTM_fuel.csv')\n",
    "astm.columns = ['Date', 'TN_retailers_seasons', 'TN_distributor_seasons',\n",
    "       'vapor_liquid_minC_retail', 'distillation_50_minC _retail',\n",
    "       'distillation_50_maxC_retail', 'vapor_pressure_maxC_retail',\n",
    "       'vapor_liquid_minC_dist', 'distillation_50_minC_dist',\n",
    "       'distillation_50_maxC_dist', 'vapor_pressure_maxC_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasoline = process_data_for_model_building(gasoline_proc, astm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Choose Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct features \n",
    "x_feats = ['grade']\n",
    "X = pd.get_dummies(gasoline[x_feats], dtype=float)\n",
    "X = sm.tools.add_constant(X)\n",
    "# convert target using get_dummies\n",
    "y = pd.get_dummies(gasoline[\"compliance_vap_liq_pressure\"], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.iloc[:,0], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is heavily imbalanced with a 26:9062 ratio of 1 to 0s. In order to find a reliable result, we need to balance these numbers with oversampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label Count '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Label Count '0': {} \\n\".format(sum(y_train==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "\n",
    "# simple resampling from your previously split data\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Label Count '1': {}\".format(sum(y_train_resampled==1)))\n",
    "print(\"Label Count '0': {} \\n\".format(sum(y_train_resampled==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a balanced dataset to build our models with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Vapor Liquid-Ratio Test Outcome ~ Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find our pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_model_pvalue(y_train_resampled, X_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These pvalues are not significant. Let's try another model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Vapor Liquid-Ratio Test Outcome ~ Tennessee Retailers Season & Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeat the process laid out above for our second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct features \n",
    "x_feats = ['TN_retailers_seasons', 'grade']\n",
    "X = pd.get_dummies(gasoline[x_feats], dtype=float)\n",
    "X = sm.tools.add_constant(X)\n",
    "# convert target using get_dummies\n",
    "y = pd.get_dummies(gasoline[\"compliance_vap_liq_pressure\"], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.iloc[:,0], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "\n",
    "# simple resampling from your previously split data\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple resampling from your previously split data\n",
    "X_test_resampled, y_test_resampled = smote.fit_sample(X_test, y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_pvalue(y_train_resampled, X_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither of our models have significant pvalues, but, since we want to understand a bit better about blah blah blah blah, we are going to choose the second model (which have slightly higher pvalues) and move on to our testing phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Chosen Model (Prediction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_resampled = logreg.predict(X_test_resampled)\n",
    "y_hat_train_resampled = logreg.predict(X_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall, Accuracy and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Precision: ', precision_score(y_hat_train_resampled, y_train_resampled))\n",
    "print('Testing Precision: ', precision_score(y_hat_test_resampled, y_test_resampled))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Training Recall: ', recall_score(y_hat_train_resampled, y_train_resampled))\n",
    "print('Testing Recall: ', recall_score(y_hat_test_resampled, y_test_resampled))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Training Accuracy: ', accuracy_score(y_hat_train_resampled, y_train_resampled))\n",
    "print('Testing Accuracy: ', accuracy_score(y_hat_test_resampled, y_test_resampled))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Training F1-Score: ',f1_score(y_hat_train_resampled, y_train_resampled))\n",
    "print('Testing F1-Score: ',f1_score(y_hat_test_resampled, y_test_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve & AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First calculate the probability scores of each of the datapoints:\n",
    "y_score = logreg.fit(X_train_resampled, y_train_resampled).decision_function(X_test_resampled)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_resampled, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLAH BLAH BLAH "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
